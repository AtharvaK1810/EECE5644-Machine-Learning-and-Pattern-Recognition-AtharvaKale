# -*- coding: utf-8 -*-
"""Ass4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wLnu-3sLGfShb09Kbhv0Uzo0YfN4trAL
"""

import os
import time
import math
import tarfile
import glob
import urllib.request
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

from sklearn.model_selection import KFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.mixture import GaussianMixture
import imageio.v2 as imageio

NAME  = "Atharva Prashant Kale"
NUID  = "002442878"
EMAIL = "kale.ath@northeastern.edu"

OUT_DIR = "a4_outputs"
FIG_DIR_Q1 = os.path.join(OUT_DIR, "q1_figs")
FIG_DIR_Q2 = os.path.join(OUT_DIR, "q2_figs")
DATA_DIR   = "bsds300_data"

os.makedirs(FIG_DIR_Q1, exist_ok=True)
os.makedirs(FIG_DIR_Q2, exist_ok=True)
os.makedirs(DATA_DIR,   exist_ok=True)

np.random.seed(5644)

def add_signature(ax, name=NAME, nuid=NUID, email=EMAIL):
    """Stamp a light signature on the current figure (bottom center)."""
    txt = f"{name} | NUID: {nuid} | {email}"
    ax.text(
        0.5,
        -0.12,
        txt,
        transform=ax.transAxes,
        ha="center",
        va="top",
        fontsize=8,
        alpha=0.8,
    )

def savefig(fig, path):
    fig.tight_layout()
    fig.savefig(path, bbox_inches="tight", dpi=200)
    plt.close(fig)


# Question 1 - SVM and MLP on concentric ring data 

def generate_ring_data(N_train=1000, N_test=10000, r_neg=2.0, r_pos=4.0, sigma=1.0):
    """
    Generate training and test data for binary classification:
        x = r_l [cos(theta), sin(theta)]^T + n
    where l in {-1, +1}, theta ~ Uniform[-pi, pi],
    and n ~ N(0, sigma^2 I_2).
    """
    def gen(N, r, label):
        theta = np.random.uniform(-math.pi, math.pi, size=N)
        base = np.stack([np.cos(theta), np.sin(theta)], axis=1) * r
        noise = np.random.normal(0.0, sigma, size=(N, 2))
        X = base + noise
        y = np.full(N, label, dtype=int)
        return X, y

    N_half_tr = N_train // 2
    N_half_te = N_test // 2

    X_tr_neg, y_tr_neg = gen(N_half_tr, r_neg, -1)
    X_tr_pos, y_tr_pos = gen(N_train - N_half_tr, r_pos, +1)
    X_te_neg, y_te_neg = gen(N_half_te, r_neg, -1)
    X_te_pos, y_te_pos = gen(N_test - N_half_te, r_pos, +1)

    X_train = np.vstack([X_tr_neg, X_tr_pos])
    y_train = np.concatenate([y_tr_neg, y_tr_pos])
    X_test  = np.vstack([X_te_neg, X_te_pos])
    y_test  = np.concatenate([y_te_neg, y_te_pos])

    # Shuffle to mix classes
    idx_tr = np.random.permutation(len(X_train))
    idx_te = np.random.permutation(len(X_test))
    return X_train[idx_tr], y_train[idx_tr], X_test[idx_te], y_test[idx_te]

def q1_run():
    print("=== Question 1: SVM and MLP on concentric rings ===")

    # 1. Generate data
    X_train, y_train, X_test, y_test = generate_ring_data(
        N_train=1000, N_test=10000, r_neg=2.0, r_pos=4.0, sigma=1.0
    )
    print(f"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

   
    # 1A. SVM with RBF kernel - hyperparameter selection
    
    print("\n[Q1 - SVM] Hyperparameter selection with K-fold CV")

    svm_pipeline = Pipeline(
        [
            ("scaler", StandardScaler()),
            ("svc", SVC(kernel="rbf")),
        ]
    )

    C_grid = [0.1, 1.0, 10.0, 100.0]
    gamma_grid = [0.01, 0.1, 1.0, 10.0]

    svm_param_grid = {
        "svc__C": C_grid,
        "svc__gamma": gamma_grid,
    }

    svm_cv = GridSearchCV(
        svm_pipeline,
        svm_param_grid,
        cv=10,
        scoring="accuracy",
        n_jobs=-1,
        return_train_score=True,
    )

    t0 = time.time()
    svm_cv.fit(X_train, y_train)
    t_svm_cv = time.time() - t0

    print(f"SVM CV completed in {t_svm_cv:.2f} seconds")
    print("Best SVM parameters:")
    print(f"  C     = {svm_cv.best_params_['svc__C']}")
    print(f"  gamma = {svm_cv.best_params_['svc__gamma']}")
    print(f"Best mean CV accuracy: {svm_cv.best_score_:.4f}")

    # Prepare heatmap data of mean CV accuracy
    mean_scores = svm_cv.cv_results_["mean_test_score"].reshape(
        len(C_grid), len(gamma_grid)
    )

    fig_hm, ax_hm = plt.subplots(figsize=(6, 5))
    im = ax_hm.imshow(
        mean_scores,
        origin="lower",
        aspect="auto",
        extent=[0, len(gamma_grid) - 1, 0, len(C_grid) - 1],
    )
    plt.colorbar(im, ax=ax_hm, label="Mean CV accuracy")
    ax_hm.set_xticks(range(len(gamma_grid)))
    ax_hm.set_xticklabels([str(g) for g in gamma_grid])
    ax_hm.set_yticks(range(len(C_grid)))
    ax_hm.set_yticklabels([str(C) for C in C_grid])
    ax_hm.set_xlabel("gamma")
    ax_hm.set_ylabel("C")
    ax_hm.set_title("Q1 SVM - 10-fold CV mean accuracy")
    add_signature(ax_hm)
    savefig(fig_hm, os.path.join(FIG_DIR_Q1, "q1_svm_cv_heatmap.png"))

    # Train best SVM on full training set and evaluate on test set
    best_svm = svm_cv.best_estimator_
    y_pred_svm = best_svm.predict(X_test)
    svm_test_acc = accuracy_score(y_test, y_pred_svm)
    svm_test_error = 1.0 - svm_test_acc
    print(f"SVM test accuracy: {svm_test_acc:.4f}, P(error) = {svm_test_error:.4f}")

    # Plot SVM decision boundary with test data
    fig_svm, ax_svm = plt.subplots(figsize=(6, 6))
    # scatter test points
    mask_neg = y_test == -1
    mask_pos = y_test == +1
    ax_svm.scatter(
        X_test[mask_neg, 0],
        X_test[mask_neg, 1],
        s=8,
        alpha=0.5,
        label="Class -1",
    )
    ax_svm.scatter(
        X_test[mask_pos, 0],
        X_test[mask_pos, 1],
        s=8,
        alpha=0.5,
        label="Class +1",
    )

    # decision boundary grid
    x_min, x_max = X_test[:, 0].min() - 1.0, X_test[:, 0].max() + 1.0
    y_min, y_max = X_test[:, 1].min() - 1.0, X_test[:, 1].max() + 1.0
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 300),
        np.linspace(y_min, y_max, 300),
    )
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = best_svm.decision_function(grid).reshape(xx.shape)
    ax_svm.contour(xx, yy, Z, levels=[0.0], colors="k", linewidths=2)

    # Manual legend entry for boundary
    boundary_proxy = Line2D([0], [0], color="k", linewidth=2, label="SVM boundary")
    handles, labels = ax_svm.get_legend_handles_labels()
    handles.append(boundary_proxy)
    labels.append("SVM boundary")
    ax_svm.legend(handles=handles, labels=labels, loc="upper right", fontsize=9)

    ax_svm.set_title(
        f"Q1 SVM decision boundary on test data\nP(error) = {svm_test_error:.4f}"
    )
    ax_svm.set_xlabel("x1")
    ax_svm.set_ylabel("x2")
    ax_svm.set_aspect("equal", "box")
    ax_svm.grid(True, alpha=0.3)
    add_signature(ax_svm)
    savefig(fig_svm, os.path.join(FIG_DIR_Q1, "q1_svm_boundary.png"))

    
    # 1B. MLP with one hidden layer - hyperparameter selection
    
    print("\n[Q1 - MLP] Hyperparameter selection with K-fold CV")

    mlp_pipeline = Pipeline(
        [
            ("scaler", StandardScaler()),
            (
                "mlp",
                MLPClassifier(
                    hidden_layer_sizes=(16,),
                    activation="tanh",   
                    max_iter=1000,       
                    solver="adam",
                    random_state=5644,
                ),
            ),
        ]
    )

    P_grid = [4, 8, 16, 32, 64]
    alpha_grid = [1e-4, 1e-3]

    mlp_param_grid = {
        "mlp__hidden_layer_sizes": [(p,) for p in P_grid],
        "mlp__alpha": alpha_grid,
    }

    mlp_cv = GridSearchCV(
        mlp_pipeline,
        mlp_param_grid,
        cv=10,
        scoring="accuracy",
        n_jobs=-1,
        return_train_score=True,
    )

    t0 = time.time()
    mlp_cv.fit(X_train, y_train)
    t_mlp_cv = time.time() - t0

    print(f"MLP CV completed in {t_mlp_cv:.2f} seconds")
    best_mlp_params = mlp_cv.best_params_
    best_hidden = best_mlp_params["mlp__hidden_layer_sizes"][0]
    best_alpha = best_mlp_params["mlp__alpha"]
    print("Best MLP parameters:")
    print(f"  hidden units P = {best_hidden}")
    print(f"  alpha          = {best_alpha}")
    print(f"Best mean CV accuracy: {mlp_cv.best_score_:.4f}")

    # Prepare line plot of mean CV accuracy versus P, for each alpha
    mean_scores = mlp_cv.cv_results_["mean_test_score"]
    params_list = mlp_cv.cv_results_["params"]

    # Build dictionary: alpha -> (P list, scores list)
    cv_summary = {}
    for mean_acc, params in zip(mean_scores, params_list):
        P = params["mlp__hidden_layer_sizes"][0]
        alpha_val = params["mlp__alpha"]
        cv_summary.setdefault(alpha_val, {"P": [], "acc": []})
        cv_summary[alpha_val]["P"].append(P)
        cv_summary[alpha_val]["acc"].append(mean_acc)

    fig_mlp_cv, ax_mlp_cv = plt.subplots(figsize=(6, 5))
    for alpha_val, dct in sorted(cv_summary.items()):
        P_arr = np.array(dct["P"])
        acc_arr = np.array(dct["acc"])
        sort_idx = np.argsort(P_arr)
        P_sorted = P_arr[sort_idx]
        acc_sorted = acc_arr[sort_idx]
        ax_mlp_cv.plot(P_sorted, acc_sorted, "-o", label=f"alpha={alpha_val:g}")

    ax_mlp_cv.set_xlabel("Hidden units P")
    ax_mlp_cv.set_ylabel("Mean CV accuracy")
    ax_mlp_cv.set_title("Q1 MLP - 10-fold CV mean accuracy vs P")
    ax_mlp_cv.grid(True, alpha=0.3)
    ax_mlp_cv.legend()
    add_signature(ax_mlp_cv)
    savefig(fig_mlp_cv, os.path.join(FIG_DIR_Q1, "q1_mlp_cv_curve.png"))

    # Train best MLP on all training data and evaluate on test set
    best_mlp = mlp_cv.best_estimator_
    y_pred_mlp = best_mlp.predict(X_test)
    mlp_test_acc = accuracy_score(y_test, y_pred_mlp)
    mlp_test_error = 1.0 - mlp_test_acc
    print(f"MLP test accuracy: {mlp_test_acc:.4f}, P(error) = {mlp_test_error:.4f}")

    # Plot MLP decision boundary with test data
    fig_mlp, ax_mlp = plt.subplots(figsize=(6, 6))
    ax_mlp.scatter(
        X_test[mask_neg, 0],
        X_test[mask_neg, 1],
        s=8,
        alpha=0.5,
        label="Class -1",
    )
    ax_mlp.scatter(
        X_test[mask_pos, 0],
        X_test[mask_pos, 1],
        s=8,
        alpha=0.5,
        label="Class +1",
    )

    proba_grid = best_mlp.predict_proba(grid)[:, 1].reshape(xx.shape)
    ax_mlp.contour(xx, yy, proba_grid, levels=[0.5], colors="k", linewidths=2)

    boundary_proxy_mlp = Line2D([0], [0], color="k", linewidth=2, label="MLP boundary")
    handles_mlp, labels_mlp = ax_mlp.get_legend_handles_labels()
    handles_mlp.append(boundary_proxy_mlp)
    labels_mlp.append("MLP boundary")
    ax_mlp.legend(handles=handles_mlp, labels=labels_mlp, loc="upper right", fontsize=9)

    ax_mlp.set_title(
        f"Q1 MLP decision boundary on test data\nP(error) = {mlp_test_error:.4f}"
    )
    ax_mlp.set_xlabel("x1")
    ax_mlp.set_ylabel("x2")
    ax_mlp.set_aspect("equal", "box")
    ax_mlp.grid(True, alpha=0.3)
    add_signature(ax_mlp)
    savefig(fig_mlp, os.path.join(FIG_DIR_Q1, "q1_mlp_boundary.png"))

    print("\nQ1 finished. Figures are in:", FIG_DIR_Q1)
    print(f"SVM test P(error) = {svm_test_error:.4f}")
    print(f"MLP test P(error) = {mlp_test_error:.4f}")


# Question 2 - GMM based image segmentation

def download_and_extract_bsds_archive():
    """
    Download the official BSDS300 images archive (~15MB) and extract it.
    """
    url = "https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz"
    archive_path = os.path.join(DATA_DIR, "BSDS300-images.tgz")

    if not os.path.exists(archive_path):
        print("Downloading BSDS300 archive...")
        urllib.request.urlretrieve(url, archive_path)
        print("Download complete.")
    else:
        print("Archive already exists, skipping download.")

    print("Extracting archive...")
    with tarfile.open(archive_path, "r:gz") as tar:
        tar.extractall(path=DATA_DIR)
    print("Extraction done.")

def get_first_bsds_image():
    """
    Finds the first available color image in the extracted directory,
    regardless of exact internal folder structure.
    """
    pattern = os.path.join(DATA_DIR, "**", "*.jpg")
    files = sorted(glob.glob(pattern, recursive=True))
    if not files:
        raise RuntimeError(f"No BSDS300 .jpg images found under {DATA_DIR}")
    return files[0]

def build_pixel_features(img, max_pixels_for_cv=40000):
    """
    For each pixel, build a 5D feature vector:
      [row_index, col_index, R, G, B]
    then normalize each feature dimension independently to [0, 1].
    Returns:
      feats_all: (N, 5) for all pixels
      feats_cv: (N_cv, 5) subset used for CV
      H, W: image shape
    """
    H, W, C = img.shape
    assert C == 3

    # row and column indices
    rows, cols = np.meshgrid(np.arange(H), np.arange(W), indexing="ij")

    # raw features
    feats = np.stack(
        [
            rows.ravel().astype(np.float32),
            cols.ravel().astype(np.float32),
            img[:, :, 0].ravel().astype(np.float32),
            img[:, :, 1].ravel().astype(np.float32),
            img[:, :, 2].ravel().astype(np.float32),
        ],
        axis=1,
    )

    # normalize each feature dimension to [0, 1]
    mins = feats.min(axis=0)
    maxs = feats.max(axis=0)
    denom = maxs - mins
    denom[denom == 0.0] = 1.0
    feats_norm = (feats - mins) / denom

    N = feats_norm.shape[0]
    if N > max_pixels_for_cv:
        idx_cv = np.random.choice(N, max_pixels_for_cv, replace=False)
        feats_cv = feats_norm[idx_cv]
    else:
        feats_cv = feats_norm

    return feats_norm, feats_cv, H, W

def gmm_cv_model_selection(feats_cv, K_grid, n_folds=5, max_iter=200):
    """
    Perform K-fold CV to select the best number of GMM components.
    Objective: maximize average validation log likelihood.
    """
    print("\n[Q2] GMM model order selection with CV")
    print(f"Number of CV samples: {feats_cv.shape[0]}")
    print(f"K candidates: {K_grid}")
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=5644)

    avg_ll_per_K = []

    for K in K_grid:
        fold_ll = []
        print(f"  Evaluating K = {K}")
        for fold_idx, (tr_idx, va_idx) in enumerate(kf.split(feats_cv)):
            X_tr = feats_cv[tr_idx]
            X_va = feats_cv[va_idx]

            gm = GaussianMixture(
                n_components=K,
                covariance_type="full",
                max_iter=max_iter,
                reg_covar=1e-6,
                random_state=5644 + fold_idx,
            )
            gm.fit(X_tr)
            ll = gm.score(X_va)  # mean log likelihood
            fold_ll.append(ll)
        mean_ll = float(np.mean(fold_ll))
        avg_ll_per_K.append(mean_ll)
        print(f"    mean validation log-likelihood = {mean_ll:.3f}")

    avg_ll_per_K = np.array(avg_ll_per_K)
    best_idx = int(np.argmax(avg_ll_per_K))
    best_K = int(K_grid[best_idx])
    print(f"Best K according to CV: {best_K}")

    return best_K, avg_ll_per_K

def segment_image_with_gmm(img, feats_all, K):
    """
    Fit a GMM with K components to all pixel features and assign
    each pixel to the most likely component.
    Returns the label image of shape (H, W).
    """
    print(f"\n[Q2] Fitting final GMM with K = {K} on all pixels")
    gm = GaussianMixture(
        n_components=K,
        covariance_type="full",
        max_iter=300,
        reg_covar=1e-6,
        random_state=5644,
    )
    gm.fit(feats_all)
    labels = gm.predict(feats_all)
    return labels.reshape(img.shape[0], img.shape[1])

def q2_run():
    print("\n=== Question 2: GMM based color image segmentation ===")

    
    download_and_extract_bsds_archive()

    
    img_path = get_first_bsds_image()
    print(f"Using image for segmentation:\n  {img_path}")

    img = imageio.imread(img_path)
    if img.ndim == 2:
        img = np.stack([img, img, img], axis=-1)
    H, W, _ = img.shape
    print(f"Image shape: {H} x {W}")

    # visualize original
    fig_orig, ax_orig = plt.subplots(figsize=(6, 6))
    ax_orig.imshow(img)
    ax_orig.set_title("Q2 - Original BSDS300 image")
    ax_orig.axis("off")
    add_signature(ax_orig)
    savefig(fig_orig, os.path.join(FIG_DIR_Q2, "q2_original_image.png"))

    #  Build pixel features
    feats_all, feats_cv, H, W = build_pixel_features(img, max_pixels_for_cv=40000)

    #  GMM model order selection
    K_grid = [2, 3, 4, 5, 6]
    best_K, avg_ll = gmm_cv_model_selection(feats_cv, K_grid, n_folds=5, max_iter=200)

    # Plot CV log likelihood vs K
    fig_cv, ax_cv = plt.subplots(figsize=(6, 4))
    ax_cv.plot(K_grid, avg_ll, "-o")
    ax_cv.set_xlabel("Number of components K")
    ax_cv.set_ylabel("Mean validation log-likelihood")
    ax_cv.set_title("Q2 GMM - CV mean log-likelihood vs K")
    ax_cv.grid(True, alpha=0.3)
    add_signature(ax_cv)
    savefig(fig_cv, os.path.join(FIG_DIR_Q2, "q2_gmm_cv_loglik_vs_K.png"))

    #  Fit final GMM and segment image
    labels = segment_image_with_gmm(img, feats_all, best_K)

    #  Build a grayscale label image with good contrast
    label_ids = np.unique(labels)
    K_final = len(label_ids)
    gray_levels = np.linspace(0, 255, K_final, dtype=np.uint8)
    label_to_gray = {lab: gray_levels[i] for i, lab in enumerate(label_ids)}
    seg_gray = np.vectorize(label_to_gray.get)(labels).astype(np.uint8)

    #  Show original and segmentation side by side
    fig_side, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(img)
    axes[0].set_title("Original image")
    axes[0].axis("off")

    axes[1].imshow(seg_gray, cmap="gray")
    axes[1].set_title(f"GMM segmentation (K = {best_K})")
    axes[1].axis("off")

    add_signature(axes[1])
    savefig(fig_side, os.path.join(FIG_DIR_Q2, "q2_segmentation_side_by_side.png"))

    print("Q2 finished. Figures are in:", FIG_DIR_Q2)
    print(f"Selected K = {best_K}")


if __name__ == "__main__":
    print("===============================================")
    print("EECE5644 Assignment 4 - Full Python Pipeline")
    print(f"Author: {NAME} | NUID: {NUID}")
    print("===============================================\n")

    q1_run()
    q2_run()

    print("\nAll tasks complete.")
    print("Output directory:", OUT_DIR)

!zip -r a4_results.zip a4_outputs
print("Zip file created successfully!")
